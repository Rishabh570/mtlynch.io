#!/usr/bin/python3

import os
import re
import urllib.error
import urllib.request

AMAZON_SHORTLINK_PATTERN = re.compile(r'https?://amzn.to/[^\?\&\"\)\s]+')
AMAZON_URL_PATTERN = re.compile(r'https?://([^\.]+\.)?amazon\.com/[^\)]*tag=[^\")]*[^\?\&\"\)\s]+')
AMAZON_SUFFIX_PATTERN = re.compile(r'/ref=.*$')


def simplify_amazon_url(url):
  return AMAZON_SUFFIX_PATTERN.sub('/', url)


def smilify(url):
  return url.replace('www.amazon.com', 'smile.amazon.com')


def find_all_matches(pattern, text):
  return [m.group() for m in pattern.finditer(text)]


def replace_affiliate_tags(path):
  with open(path, encoding='utf-8') as f:
    contents = f.read()
    for amazon_url in find_all_matches(AMAZON_URL_PATTERN, contents):
      simple_url = smilify(simplify_amazon_url(amazon_url))
      contents = contents.replace(amazon_url, simple_url)
    for short_link in find_all_matches(AMAZON_SHORTLINK_PATTERN, contents):
      try:
        url = urllib.request.urlopen(short_link).url
      except urllib.error.HTTPError as ex:
        url = ex.geturl()
      simple_url = smilify(simplify_amazon_url(url))
      contents = contents.replace(short_link, simple_url)
  with open(path, 'w', encoding='utf-8') as f:
    f.write(contents)


def remove_links(content_root):
  for root, dirs, files in os.walk(content_root):
    for f in files:
      if not f.endswith('.md'):
        continue
      replace_affiliate_tags(os.path.join(root, f))

if __name__ == '__main__':
  remove_links('./content')